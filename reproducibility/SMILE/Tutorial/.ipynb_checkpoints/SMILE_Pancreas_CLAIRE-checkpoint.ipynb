{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import scanorama\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy.sparse as sps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os.path import join\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "cur_dir = Path(os.getcwd())\n",
    "sys.path.append(str(cur_dir.parent.absolute()))\n",
    "sys.path.append(str(cur_dir.parent.parent.parent.absolute()))\n",
    "\n",
    "from moco.kbet import calculate_kbet\n",
    "from moco.utils import py_read_data, load_meta_txt\n",
    "from moco.evaluation import scib_process \n",
    "from moco.preprocessing import embPipe\n",
    "\n",
    "from SMILE import SMILE\n",
    "from SMILE.SMILE import SMILE_trainer\n",
    "from SMILE.utils import py_read_data, load_meta_txt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "from scib_eval import scib_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reading cost time 0.0397 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'batchlb' as categorical\n",
      "... storing 'CellType' as categorical\n",
      "/home/yxh/anaconda3/envs/torch171/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# prepare datasets\n",
    "# ================================\n",
    "data_root = '/home/yxh/gitrepo/Batch-effect-removal-benchmarking-master/Script/sapling/GLOBE/data'\n",
    "\n",
    "dno = 'Pancreas'\n",
    "dataset_dir = join(data_root, dno)\n",
    "# data_name = 'b1_exprs'\n",
    "batch_key = 'batchlb'\n",
    "label_key = 'CellType'\n",
    "\n",
    "sps_x, gene_name, cell_name = py_read_data(dataset_dir, 'myData_pancreatic_5batches')\n",
    "df_meta = load_meta_txt(join(dataset_dir, 'mySample_pancreatic_5batches.txt'))\n",
    "df_meta[label_key] = df_meta['celltype']\n",
    "\n",
    "# ================================\n",
    "# preprocessing\n",
    "# ================================\n",
    "min_cells=3\n",
    "scale_factor=1e4\n",
    "n_hvgs=2000\n",
    "n_pcs=50\n",
    "n_neighbors=20\n",
    "\n",
    "adata = sc.AnnData(sps.csr_matrix(sps_x.T))  # transposed before\n",
    "adata.obs_names = cell_name\n",
    "adata.var_names = gene_name\n",
    "adata.obs[batch_key] = df_meta.loc[cell_name, batch_key]\n",
    "# change the batch from number to string \n",
    "# adata.obs[batch_key] = adata.obs[batch_key].apply(lambda x: f'Batch{int(x)+1}')\n",
    "adata.obs[label_key] = df_meta.loc[cell_name, label_key]\n",
    "\n",
    "\n",
    "sc.pp.filter_genes(adata, min_cells=min_cells) \n",
    "sc.pp.normalize_total(adata, target_sum=scale_factor)\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=min(adata.shape[1]-1, n_hvgs), \n",
    "#                             min_mean=0.0125, max_mean=3, min_disp=0.5,\n",
    "                            batch_key=batch_key)\n",
    "\n",
    "adata_hvg = adata[:, adata.var.highly_variable].copy()\n",
    "# adata_hvg.write(join(dataset_dir, f'NormLog-{dno}.h5ad'))  # for clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_hvg_scale = adata_hvg.copy()\n",
    "\n",
    "X = adata_hvg.X\n",
    "X = X.A if sps.issparse(X) else X.copy()\n",
    "\n",
    "scaler = StandardScaler()##scaling\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "adata_hvg_scale.X = sps.csr_matrix(X)\n",
    "# adata_hvg_scale.write(join(dataset_dir, f'NormLogScale-{dno}.h5ad'))  # for clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxh/anaconda3/envs/torch171/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# Training\n",
    "# ================================\n",
    "def weights_init(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "X = adata_hvg.X \n",
    "X = X.A if sps.issparse(X) else X.copy()\n",
    "\n",
    "scaler = StandardScaler()##scaling\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "##3. Use SMILE for single-cell RNA-seq data\n",
    "X_all_tensor = torch.tensor(X).float()\n",
    "\n",
    "# training params\n",
    "clf_out = 25\n",
    "batch_size = 512\n",
    "lr = 1e-4\n",
    "num_epoch = 80\n",
    "EPS = [80] # 10, 20, 40, 80\n",
    "\n",
    "\n",
    "net = SMILE.SMILE(input_dim=X.shape[1],clf_out=clf_out)   # input_dim, \n",
    "net.apply(weights_init) ##initialize weights, only once\n",
    "\n",
    "# log\n",
    "log_dir = f'../outputs/{dno}-0.01'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "SMILE_trainer(X,\n",
    "              net,\n",
    "              lr=lr,   # 1e-2, 1e-4 tried\n",
    "              batch_size = batch_size, \n",
    "              num_epoch=num_epoch,\n",
    "              f_temp = 0.05, \n",
    "              p_temp = 0.15,\n",
    "              log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# inference\n",
    "# ================================\n",
    "ADAS = []\n",
    "for ep in [80]:\n",
    "    # loading checkpoint\n",
    "    ckpt_name = 'checkpoint_{:04d}.pth.tar'.format(ep)\n",
    "    checkpoint = torch.load(join(log_dir, 'weights1', ckpt_name))\n",
    "\n",
    "    net.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    print(\"=> loaded checkpoint: {}\"\n",
    "          .format(ckpt_name))\n",
    "\n",
    "    net.to(torch.device(\"cpu\"))\n",
    "    y_pred = np.zeros((X.shape[0],128))\n",
    "\n",
    "    for j in range(X.shape[0]//batch_size+1):\n",
    "        pred = net.encoder(X_all_tensor[j*batch_size:(j+1)*batch_size, :])  # 再看到这里，才发现SMILE是个完整的MocoV2模型\n",
    "        pred = torch.Tensor.cpu(pred).detach().numpy()\n",
    "        y_pred[j*batch_size:(j+1)*batch_size, :]=pred\n",
    "\n",
    "    # convert to scanpy obj\n",
    "    ada_tmp = embPipe(y_pred, df_meta)\n",
    "    ada_tmp.obsm[\"X_emb\"] = ada_tmp.X\n",
    "    ADAS.append(ada_tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch171",
   "language": "python",
   "name": "torch171"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
